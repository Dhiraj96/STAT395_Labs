The format of this lab follows the same format as the previous
ones. Your goal is to predict the value of the third column
(which will be missing on the test set) using the techniques
we have learned so far.

One thing that you are not allowed to use are neural networks.
We will see how to use this with images next class.

# Set-up

Read in the following libraries and to load the metadata about
the flowers:

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(viridis)
library(stringi)
library(glmnet)

flowers <- read_csv("https://statsmaths.github.io/ml_data/flowers_17.csv")
```

To get the actual data for the images, you'll have to download the following
file.

- https://drive.google.com/open?id=0B6_9OUDRaPQsc21sZXQ2Q250bFk

Once these are downloaded, you'll have to run something like this:

```{r}
x64 <- read_rds("~/Desktop/flowers_17_x64.rds")
```

If, for example, the file sits on your Desktop and you have a Mac. If you have
trouble with any of this, please let me know as soon as possible.


# Lab 18

We first start small by collapsing the data and applying an elastic net to the data. We see that there is not a large difference in deviance between the min and 1se lambda. Looking at the classification rates, we see that a simple elastic model with the minimum lambda causes our model to heavily overfit to the training data. 
```{r}
X <- t(apply(x64,1,cbind))
X_train <- X[flowers$train_id == "train",]

y <- flowers$class
y_train <- y[flowers$train_id == "train"]

model1 <- cv.glmnet(X_train, y_train, family = "multinomial", nfolds = 3)
beta <- coef(model1, s = c(model1$lambda.1se, model1$lambda.min))

plot(model1)

flowers$predict_min <- predict(model1, newx = X, type="class", s="lambda.min")
flowers$predict_1se <- predict(model1, newx = X, type="class", s="lambda.1se")

tapply(flowers$predict_min == y, flowers$train_id, mean)
tapply(flowers$predict_1se == y, flowers$train_id, mean)

```

Let us try to see where the model misclassifies our flowers for the validation set using the following confusion matrix. 
```{r}
table(y = flowers$predict_min[flowers$train_id == "valid"], x = flowers$class[flowers$train_id == "valid"])

```


Next, I attempt to convert our RGB color scheme into a HSV color scheme and apply a glmnet model to our new data to see if prediction is improved at all. The first block creates a matrix hsv with each of the continuous hsv values. The second discretizes them into small set of fixed colors (like in class), setting any value below 0.2
```{r}

color_vals <- c(hsv(1, 0, seq(0, 1, by = 0.2)), 
                hsv(seq(0, 0.99, by = 0.01), 1, 1))

X_hsv <- matrix(0, ncol = length(color_vals),
                   nrow = nrow(flowers))

for(i in seq_len(nrow(flowers))){
  red <- as.numeric(x64[i,,,1])
  green <- as.numeric(x64[i,,,2])
  blue <- as.numeric(x64[i,,,3])
  hsv <- t(rgb2hsv(red, green, blue, maxColorValue = 1))
  
  color <- rep("#000000", nrow(hsv))
  
  index <- which(hsv[,2] < 0.2)
  color[index] <- hsv(1,0,round(hsv[index,2]*5)/5)
  
  index <- which(hsv[,2] >0.2 & hsv[,3] > 0.2)
  color[index] <- hsv(round(hsv[index,1],1),1,1)
  
  X_hsv[i,] <- table(factor(color, levels=color_vals))
}

```


From the model below, we see that our validation accuracy has dropped significantly from .4 to .3 but there is much less evidence of the data being overfit to the training set data. I suspect this is due to there being many classes of flowers with the same colors, causing a higher degree of misclassification.
```{r}
y <- flowers$class

X_train <- X_hsv[flowers$train_id == "train",]
X_valid <- X_hsv[flowers$train_id == "valid",]
y_train <- y[flowers$train_id == "train"]
y_valid <- y[flowers$train_id == "valid"]

model2 <- cv.glmnet(X_train, y_train, family = "multinomial",
                   alpha = 0.2)
beta_2 <- coef(model2, s = c(model2$lambda.2se, model2$lambda.min))

plot(model2)

flowers$predict_min_2 <- as.numeric(predict(model2, newx = X_hsv, type="class", s="lambda.min"))
flowers$predict_1se_2 <- as.numeric(predict(model2, newx = X_hsv, type="class", s="lambda.1se"))

tapply(flowers$predict_min_2 == y, flowers$train_id, mean)
tapply(flowers$predict_1se_2 == y, flowers$train_id, mean)

```

Next, let us try introducing texture to the model. We do this by first converting the image into grayscale and calculating mean edge and mean edge in the middle of out picture. Just to have an idea of our results, I took an image and displayed it below. 
```{r}
mean_edge <- rep(0, nrow(flowers))
for(i in seq_len(nrow(flowers))){
  bw <- (x64[i,,,1] + x64[i,,,2] + x64[i,,,3]) / 3
  edge <- abs(bw[-1,-1]- bw[-nrow(bw), -ncol(bw)])
  mean_edge[i] <- mean(edge>0.05)
  
}

plot(0,0,xlim=c(0,1),ylim=c(0,1),axes= FALSE,type = "n")
rasterImage(bw,0,0,1,1)


mean_edge_mid <- rep(0, nrow(flowers))

for (i in seq_len(nrow(flowers))) {
  bw <- (x64[i,,,1] + x64[i,,,2] + x64[i,,,3]) / 3
  edge <- abs(bw[-1,-1] - bw[-nrow(bw),-ncol(bw)])
  mean_edge_mid[i] <- mean(edge[20:44,20:44] > 0.05)
}


```

Finally, I combined our data and used a glmnet model. The validation rate shows to be better than all our previous models. 
```{r}
X_edge <- cbind(X, X_hsv, mean_edge, mean_edge_mid)
y <- flowers$class

X_train <- X_edge[flowers$train_id == "train",]
X_valid <- X_edge[flowers$train_id == "valid",]
y_train <- y[flowers$train_id == "train"]
y_valid <- y[flowers$train_id == "valid"]


model3 <- cv.glmnet(X_train, y_train, family = "multinomial",
                   alpha = 0.2)
plot(model3)

flowers$predict_min_3 <- as.numeric(predict(model3, newx = X_edge, type="class", s="lambda.min"))
flowers$predict_1se_3 <- as.numeric(predict(model3, newx = X_edge, type="class", s="lambda.1se"))

tapply(flowers$predict_min_3 == y, flowers$train_id, mean)
tapply(flowers$predict_1se_3 == y, flowers$train_id, mean)

```

Looking at the confusion matrix for this model, wee see that many flowers with the same color/shape are the main culprit of the misclassification rates. 
```{r}
table(y=flowers$predict_min_3[flowers$train_id == "valid"], x=flowers$class[flowers$train_id == "valid"])

```


# Submission

The code below assumes that you have added a prediction named
`class_pred` to every row of the dataset.

```{r}
flowers$class_pred <- flowers$predict_min_3
submit <- select(flowers, obs_id, class_pred)
write_csv(submit, "class18_submit.csv")
```

Now, upload this file (ends with ".Rmd"), the HTML output
(ends with ".nb.html" or ".html"), and the csv file to
GitHub.
